% MSc dissertation example file, February 2022
%
% Leave one of the documentclass lines uncommented to match your degree.
% You may remove the logo option if it causes problems.
% Do not change any other options.
% \documentclass[logo,msc,adi]{infthesis}         % Adv Design Inf
% \documentclass[logo,msc,ai]{infthesis}          % AI
% \documentclass[logo,msc,cogsci]{infthesis}      % Cognitive Sci
% \documentclass[logo,msc,cs]{infthesis}          % Computer Sci
% \documentclass[logo,msc,cyber]{infthesis}       % Cyber Sec
% \documentclass[logo,msc,datasci]{infthesis}     % Data Sci
% \documentclass[logo,msc,di]{infthesis}          % Design Inf
\documentclass[logo,msc,dsti]{style/infthesis}    % Data Sci TI
% \documentclass[logo,msc,inf]{infthesis}         % Informatics
% \documentclass[logo,msc]{infthesis}             % degree unspecified, do not change except to add your degree
%%%%%%%%%%%%%%%%%%%%%%%%
% Understand any problems and seek approval before assuming it's ok to remove ugcheck.
\usepackage{style/msccheck}

% Include any packages you need below, but don't include any that change the page
% layout or style of the dissertation. By including the ugcheck package above,
% you should catch most accidental changes of page layout though.
\usepackage{microtype} % recommended, but you can remove if it causes problems
\usepackage{nolbreaks}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{xurl}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{gnuplottex}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usepackage{svg}
\usepackage{pdfpages}
\usepackage{caption}
\usepackage{listings}
\usepackage[ruled]{algorithm2e}
\usepackage{csquotes}
\usepackage[british]{babel}
\usepackage[backend=biber,urldate=long,dateabbrev=false]{biblatex}

% More nicely style url/xurl links
\hypersetup{colorlinks=true,linkcolor=blue,filecolor=blue,urlcolor=blue,citecolor=blue}

% Adding urn, hal, and hdl nice links to the biliography that show the identifier but
% not the https://.../ part while still linking to the appropriate https://.../<id>
% part, in a similar way that Biblatex automatically handles DOI links
\DeclareFieldFormat{eprint:urn}{%
	\textsc{urn\addcolon\space}
	\ifhyperref
	{\href{http://www.nbn-resolving.org/#1}{\nolinkurl{#1}}}
	{\nolinkurl{#1}}}
\DeclareFieldFormat{eprint:hal}{%
	\textsc{hal\addcolon\space}
	\ifhyperref
	{\href{https://uca.hal.science/#1}{\nolinkurl{#1}}}
	{\nolinkurl{#1}}}
\DeclareFieldFormat{eprint:hdl}{%
	\textsc{hdl\addcolon\space}
	\ifhyperref
	{\href{https://hdl.handle.net/#1}{\nolinkurl{#1}}}
	{\nolinkurl{#1}}}

% Allow more line breaks in long links in the Bibliography
\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}

% Without this we get BIBLIOGRAPHY in all caps in page headings, which is
% inconsistent with the rest of the styles (probably since they're designed
% for bibtex rather than biblatex which we use here
\defbibheading{bibliography}[\bibname]{%
\chapter*{#1}\addcontentsline{toc}{chapter}{\bibname}%
\markboth{#1}{#1}}

% The one and only biliography file
\addbibresource{main.bib}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{  
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\bfseries\footnotesize\linespread{0},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                               
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=lines,
    aboveskip=20pt,
    belowskip=15pt
}

\lstset{style=mystyle}
\captionsetup[lstlisting]{font={stretch=1.5,sf}}

% A "descitem" item for a bold "mini heading" component
\newcommand\descitem[1]{\item{\bfseries #1}\\}

% Footnotes should be single spaced, but infthesis redefines \footnote which
% then causes biblatex to throw a warning. So we have commended out that
% redefinition, but use our own command to enfore the singlespacing
\newcommand{\singlespacedfootnote}[1]{{\singlespace\footnote{#1}}}

\begin{document}
\begin{preliminary}

\title{OpenTTDLab: A reusable Python framework for repeatable, replicable, \& reproducible experiments using OpenTTD}

\author{Michal Charemza}

\date{\today}

\abstract{OpenTTD is an open source business simulation game based on the 1994 game Transport Tycoon Deluxe, and in spite of being designed for recreation, OpenTTD has been used in a number of academic studies. However, many of these studies have problems regarding the repeatability, replicability, or reproducibility of their experiments. In this dissertation, I present OpenTTDLab, a reusable Python framework I created that allows OpenTTD to be used in a way that avoids many of these problems. I show proof-of-concept results using this framework that I argue are repeatable, replicable, and reproducible; thus, giving evidence that OpenTTDLab can be a useful tool in future research.}

\maketitle

\newenvironment{ethics}
   {\begin{frontenv}{Research ethics approval}{\LARGE}}
   {\end{frontenv}\newpage}

\begin{ethics}
This project was planned in accordance with the Informatics Research
Ethics policy. It did not involve any aspects that required approval
from the Informatics Research Ethics committee.
\standarddeclaration
\end{ethics}

\begin{acknowledgements}

Firstly thank you to Chris Sawyer, the original author of Transport Tycoon Deluxe: without you this project would not have existed. Then of course thank you to all the contributors of OpenTTD over its 20 year history. And of these, thank you especially to Patric Stout (A.K.A. TrueBrain), who not only gave advice and what I interpreted to be light blessing to this project, but also wrote the OpenTTD save game parser that OpenTTDLab originally forked from.

Thank you also to Ian Earle (A.K.A. BasicBeluga) who found an issue in the documentation of OpenTTDLab and submitted a fix for it. While this was a small change, I took it as evidence that what I was creating stood a chance of being useful, and pushed me to continue.

Thank you to my supervisor, Michael Hermann; his ongoing advice throughout this project has been invaluable. I thank him especially for making sure that I have some results to show and discuss rather than getting lost in just making OpenTTDLab, and I thank him for guiding me towards what I hope is now a reasonable written dissertation. And I would say he rolled with the changes to the project especially well: I shifted its focus several times between proposal and final output, and I felt supported every step of the way.

And of course thank you to my husband, Matthew Beach, an amazingly loving partner, and whose support throughout this project has been beyond invaluable.

\end{acknowledgements}

\tableofcontents

\end{preliminary}


\chapter{Introduction}
\label{chapter:introduction}

OpenTTD \cite{openttd} is an open source real time strategy (RTS) simulation game based on  Chris Sawyer's 1994 game Transport Tycoon Deluxe. The aim of the game is to successfully run a business by constructing networks of roads, railways, airports and ports, along with their respective vehicles, trains, planes and ships, in order to transport people and goods in exchange for money. OpenTTD is played on a simulated landscape as can be seen in Figure \ref{figure:introduction-screenshot}.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{assets/openttd_screenshot.png}
\caption{A small section of an OpenTTD version 13.4 game showing parts of a rail, road, and sea transportation network, as well as several industries. At the top is the control bar through which the player takes actions or see more details on the state of the game, such as finances.}
\label{figure:introduction-screenshot}
\end{figure}

OpenTTD was created as a game for recreation: it allows a single player to play in a non-competitive world building mode; multiple human players playing cooperatively or competitively; and so-called AI players, which via custom code that controls a company, provide opponents for humans to play against. However, it is remarkably flexible: it has been successfully used as a tool to research algorithms including artificial intelligence (AI), machine learning (ML), and anomaly detection algorithms where OpenTTD AI(s) play without human player involvement \cite{beuneker2019autonomous, bijlsma2014evolving, konijnendijk2015mcts, lakomy2020railroad, rios2009trains, wisniewski2011artificial, volna2017fuzzy}, scalability and mobile applications \cite{jiang2018mirroring}, and as a teaching aid for concurrency in computer programs \cite{HansenMuprhie2018, marmorstein2015teaching}\singlespacedfootnote{The work of Hansen \& Murphie \cite{HansenMuprhie2018} refers to an \emph{OpenTTD lab}, which is a set of scenarios and exercises for students to follow. It is not directly related to the OpenTTDLab presented here.} and for supply chain and logistics management \cite{doi:10.1080/10494820.2016.1242503}.

Simulation games have a long history of being used to inform government policies; see Raghothama \& Meije \cite{raghothama2013review} for a summary that focuses on logistics and simulation. In spite of this, no reference of using OpenTTD in this way has been found. A reason in  Raghothama \& Meije \cite{raghothama2013review} is suggested: while some of the transportation aspects of the networks in OpenTTD are realistic, its economic model is not.

Of all these possible uses for OpenTTD, using OpenTTD as a tool for researching algorithms is the primary focus of the current work, and specifically the focus is the development of a reusable tool, OpenTTDLab, that augments the existing features of OpenTTD to improve its ability to be used in repeatable, replicable, \& reproducible research where OpenTTD AI(s) play without human player involvement. Given what is now the well-known replication crisis\singlespacedfootnote{The replication crisis is also known as the replicability crisis, the reproducibility crisis, and the credibility crisis.} \cite{ioannidis2005most, baker20161}, including in computer science \cite{dalle2012reproducibility, CollbergChristianProebsting2016}, if there are problems with how such research has been conducted using OpenTTD, as I will argue, then the potential usefulness of such a tool for future research is clear.

As a secondary and more speculative aim, it is hoped that the work here allows for OpenTTD to be investigated as a tool to simulate supply chain or transportation networks that could ultimately inform government policies. However, the nature of the correspondence between OpenTTD and what it is simulating must be determined, or in other words it must be \emph{validated} \cite{doi:10.1177/1046878198291003}.

\section{The 3Rs}
\label{section:define-3rs}

The terms \emph{repeatability}, \emph{replicability}, and \emph{reproducibility}, the so-called \emph{3Rs}, unfortunately do not historically have universally agreed meanings \cite{plesser_reproducibility_2018}. Even the highly-cited 2016 Nature survey \cite{baker20161}, which reported that around 50\% of scientist believe there is a substantial reproducibility crisis, uses the terms reproducible and replicable seemingly interchangeably. And both helpfully and confusingly, the Association for Computing Machinery (ACM) in 2020 swapped their definitions reproducibility and replicability to align with the broader scientific community \cite{association_for_computing_machiner_new_2020}.

Choosing what I believe to be an authoritative source, I primarily use the post-swap ACM definitions from its \emph{Artifact Review and Badging Version 1.1} \cite{association_for_computing_machiner_artifact_2020}; these appear to broadly align with the definitions of \emph{The Turing Way} \cite{turingway2022}, a collaborative guide to reproducible data science, spearheaded by the UK's national institute for data science and artificial intelligence, a similarly authoritative source.

The ACM definition is given in terms of artifacts:

\begin{quote}
\begin{description}
\item[Artifact]

\ldots we mean a digital object that was either created by the authors to be used as part of the study or generated by the experiment itself. For example, artifacts can be software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results.
\end{description}
\end{quote}
and the definitions of the 3Rs are:

\begin{quote}
\begin{description}
\item[Repeatability] (Same team, same experimental setup)

The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation.
\item[Reproducibility] (Different team, same experimental setup)

The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author’s own artifacts.

\item[Replicability] (Different team, different experimental setup)

The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently.
\end{description}
\end{quote}

These are not independent properties: reproducibility is unlikely to be achieved without repeatability \cite{hill2022reproducibility}, and replicability would be difficult to interpret in the absence of reproducibility \cite{nuijten2018verify}. While this highlights how the definitions are intertwined and therefore complex, it also means that a tool that eases repeatability eases reproducibility, a tool that eases reproducibility eases replication, and transitively a tool that eases repeatability indirectly eases replicability.

While ACM's definitions are clear in some respects, there is vagueness, especially in terms of what \emph{same result} means \cite{hill2022reproducibility}. In general, results can be said to be reproduced even if they are not exactly the same, i.e. not \emph{bitwise} identical. For example, in simulation studies if the original random seeds are not available but all other artifacts are, then results can still be said to be replicated if statistical results are the same \cite{luijken2024replicability}. Where appropriate I make a distinction between bitwise and non-bitwise reproducibility.

Note that the definition of artifact is limited to objects created by the authors; if research uses OpenTTD then it appears the research can be deemed replicated without OpenTTD itself having to be developed independently, simply because the it was already available; to me this seems an unexpected loophole in the definitions. Similarly to how reproducibility can be bitwise or non-bitwise, if we extend the ACM definition of artifact to include other software used to generate the results, as I would argue we should, there appears to be degrees of replication that depend on how much of the original software used to run the simulation has been developed independently. It is beyond the scope of this project to deal with these issues in general, other than to ensure clarity as to what is or can be developed independently when discussing replication. In spite of the crisis being well known for over a decade, even as of May 2024 the replication of simulation studies is a ``novel endeavor'', 
and there are ``no set criteria to assess the alignment of replicated simulation results with the original results'' \cite{luijken2024replicability}; so I argue this a reasonable limitation to the scope of this project.

% \section{Barriers in achieving the 3Rs}

% The aim of OpenTTDLab is to help researchers achieve the 3Rs when using OpenTTD to research algorithms or, possibly, use OpenTTD to simulate transportation or logistic networks. As such it's important to note in general what barriers have been noted in similar research to achieving these.  Since the 3Rs are linked, de facto building on one another, it is unsurprising that barriers to achieving them are similarly linked, i.e. something that prevents one can prevent another. Here I summarise relateven barriers to achieving the 3Rs.

% Manual steps.

% Repeatability of computer simulations on the face of it should be the easiest to achieve, a researcher should \emph{reliably} repeat here own computation accor

% In terms or reproducing

% Through attempts to replicate statistical simulation studies, Luijken K et al. \cite{luijken2024replicability} construct a number of barriers to replication of studies. These 

% Why these barriers have not been overcome, especially in terms of the barriers to ensuring research is replicatable and repeatable, is usually due to a combination of their cost \cite{hernandez2023repeatability} and not sufficient reward. Reproducibility as a service has been.

% High cost is suggested, together . but the project here suggests an alternative: a tool to reduce that cost. Interestingly the tools used seems to be not studied. The closest things are preferring open source to prioprietry, and so-called reproducibility as a service. An Open Source framework for reproducible research appears to be novel.

\section{3Rs issues of existing OpenTTD research}

The aim of OpenTTDLab is to help studies that run OpenTTD to generate results by writing and running AIs without a human player; 7 existing studies have been found that do this\singlespacedfootnote{Found from the fist 100 results of a search for "OpenTTD" in Google Scholar in July 2024}: the work of Beuneker et al \cite{beuneker2019autonomous}, Bijlsma \cite{bijlsma2014evolving}, Konijnendijk \cite{konijnendijk2015mcts}, {Lakom{\`y} \cite{lakomy2020railroad}, Rios and Chaimowicz \cite{rios2009trains}, Wisniewski and Witt \cite{wisniewski2011artificial}, and Volna \cite{volna2017fuzzy}. Informally reviewing these in terms of how they have or have not seemingly achieved repeatability (the domain of the original researchers) or help achieve reproducibility and replicability (the domain of independent researchers), I found 7 problems. For brevity not all problems for all studies are listed.

\begin{itemize}

\begin{descitem}{Issue 1: Insufficient artifacts supplied to reproduce or analyse results}
One of the most complex and impressive studies is the work of Bijlsma \cite{bijlsma2014evolving}, using dynamic scripting to evolve the code of an OpenTTD AI using a genetic algorithm. In total it seems that there were at least 1250 experiments in total: 50 experiments each for 25 generations of the genetic algorithm, and after each of these in game metrics must have been extracted. There is nothing in OpenTTD that allows this to be done automatically, and it seems infeasible to have run this manually, especially because the AI code would be different each generation. The study does not explain how this was done, and so I suspect there were artifacts created not even mentioned in the study.

The work of Konijnendijk \cite{konijnendijk2015mcts} more directly suggests artifacts were created: additions to the source code of OpenTTD itself. Although described in a high level, the code of these is not available.
\end{descitem}

\begin{descitem}{Issue 2: Low or unknown number of repetitions}
Where there is no one-size fits all number to how many repetitions an experiments must have, some of the studies I would argue have repetitions that are so few they call into question the results that are based on them. The 2 experiments of Rios and Chaimowicz \cite{rios2009trains} were run 7 times each, and the experiments of Wisniewski and Witt \cite{wisniewski2011artificial} were run just 3 times for example.

The work of Volna \cite{volna2017fuzzy} is particularly of note: it does not list the number of repetitions, or even suggest there was more than one.
\end{descitem}
\begin{descitem}{Issue 3: Non-comparable repetitions}
Rios and Chaimowicz \cite{rios2009trains} ran a total of 14 repetitions over 2 experiments, but not for the same amount of in-game time. This calls into question whether these were actually repetitions of the experiment.
\end{descitem}
\begin{descitem}{Issue 4: Manual repetitions of experiments and extraction of results}
No study explicitly stated that their experiments were run in an automated mechanism, and given the low number and non-comparable repetitions of some of the works, I suspect they were run manually, in that OpenTTD was run through the graphical interface and configured through its graphical user interface, which I suspect is tedious and error prone.

However, the study of {Lakom{\`y}} \cite{lakomy2020railroad} explicitly describes the manual mechanism uses to install and configure OpenTTD an experiment. Having a description is excellent in terms of supporting reproduction of results, but the fact it is manual is high cost in terms of time to try to replicate its reported 50 repetitions.
\end{descitem}

\begin{descitem}{Issue 5: Insufficient detail on the version or configuration of OpenTTD}
Bijlsma \cite{bijlsma2014evolving} does not mention the version of OpenTTD (or the version of OpenGFX---as will be discussed a component that affects screenshots)

OpenTTD has a wide array of configuration options that can affect results.
\end{descitem}
\begin{descitem}{Issue 6: No random seeds to bitwise reproduce results}
Possible a special case of configuration, but it merits special attention since to gain statistical results typically experiments would be run for a fixed configuration over a range of random seeds. However, only one study reported the random seeds used, Beuneker et al. \cite{beuneker2019autonomous}, including a discussion of them. This impacts repeatability, but specifically bitwise reproducibility: without the random seeds it is impossible to reproduce the results exactly.
\end{descitem}

\begin{descitem}{Issue 7: Insufficient or unclear detail of the algorithms to replicate results}
All of the studies included some high level detail of the algorithms implemented, but I found it difficult to judge if it was sufficient within the time constraints of the current work.

However, even if there is sufficient detail, in all cases bar one they are not presented in a way that I would argue is ideal for replication: the detail is spread throughout the work, and not presented as a clear set of algorithms that be replicated. The exception is the work of {Lakom{\`y}} \cite{lakomy2020railroad}: it includes many algorithms each of them explained and discussed, although they are presented as code rather than pseudo-code, and so if used could muddy the water between reproducing and replicating the results.
\end{descitem}

\end{itemize}

Informally reviewing by reading the papers is certainly not as strong evidence as actually attempting to reproduce or replicate results, but an independent researcher reading through a draft manuscript has been given as a recommendation to find issues before publication with the strongest of the 3Rs, replicability \cite{luijken2024replicability}, so I argue it is reasonable to extend the process to all of the 3Rs and to base further actions, such as the creation of a framework to help avoid such issues, off of such a review. While my review focused on OpenTTD, it appears to be related wider problem in simulations in general: ``many published works based on simulation still fail to meet the minimum conditions to ensure reproducibility'' \cite{dalle2012reproducibility}.

\section{The 4\texorpdfstring{\textsuperscript{th}}{th} R: reusability}

There is a 4\textsuperscript{th} concept given alongside ACM's definitions \cite{association_for_computing_machiner_new_2020}, not so much property of the results that the 3Rs focus on, but a property of the artifacts used to generate the results.

\begin{quote}
\begin{description}
\item[{[}Reusability{]}] 
The artifacts associated with the paper are of a quality that significantly exceeds minimal functionality. [...] they are very carefully documented and well-structured to the extent that reuse and repurposing is facilitated. In particular, norms and standards of the research community for artifacts of this type are strictly adhered to. 
\end{description}
\end{quote}
Repeatability, reproducibility and replicability are all in the realm of a single result, but \emph{reusability} concerns itself with the use of the artifact to generate results of different-but-related experiments, possibly by other researchers. It is unfortunately often an afterthought in scientific software as it does not directly concern the current results the software is used to generate; but it has been argued that it can help reproducibility, and even increase the impact of the original work \cite{benureau2018re}. It is clear that a framework that helps with the 3Rs should itself be reusable and encourage reusable artifacts in work that uses it.

\section{Chapter outline}

The following chapters explain how OpenTTDLab helps experiments avoid issues with the 3Rs, especially the 7 issues listed, and in a reusable way. Chapter \ref{chapter:openttd-model-and-abilities} gives more background into OpenTTD: specifically what properties of OpenTTD make it suitable and interesting enough to research, what limitations it has, and at a high level how a framework could overcome at least some of these limitations. Chapter \ref{chapter:openttdlab-design-process-and-features} explains the design process I followed to create such a framework---the process itself provides an argument that it achieves its aims; and gives an overview of how its features help experiments be repeatable, reproducible, and albeit maybe indirectly, replicable. 

Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling} are the experimental chapters that contain results of using OpenTTDLab: providing more evidence that there is a problem with existing OpenTTD research, and providing more direct evidence that OpenTTDLab can help with the 3Rs. Specifically, Chapter \ref{chapter:experiments-attempt-at-reproducing} explains how I attempted to reproduce existing published results, and while I did extract results, they did not match the ones originally published. Chapter \ref{chapter:experiments-attempt-at-reproducing} also shows that it is relatively straightforward to use OpenTTDLab to run experiments and extract results from them, and argues that these should be repeatable and reproducible. Chapter \ref{chapter:experiments-simple-parameterised-ai} gives the details of a basic OpenTTD AI that I constructed, and easily used with OpenTTDLab to extract results that appear to have real-world meanings, thus giving some evidence that by using OpenTTDLab, OpenTTD could be used to simulate the real world as part of the secondary aim of this work as mentioned earlier. \nolbreaks{Chapter \ref{chapter:experiments-simple-parameterised-ai}} also contains a clear description of the AI, and so should be replicable in the sense that another author should be able to construct a similar enough OpenTTD AI to replicate the results, possibly using OpenTTDLab. Chapter \ref{chapter:experiments-scaling} contains the results of a basic exploration of the performance of OpenTTD, showing how features added help repeatability.

Chapter \ref{chapter:discussion} contains a discussion of what was found, summarises the limitations of the work, and explores the possible next steps. This dissertation finishes with a few concluding remarks in \nolbreaks{Chapter \ref{chapter:conclusion}}.

\chapter{OpenTTD: Its model and abilities}
\label{chapter:openttd-model-and-abilities}

A game of OpenTTD is made up of a rectangular world with one or more companies in it, typically working in competition against each other. A company is paid for transporting goods and passengers between certain combinations of industries and towns, and in order to do so it needs to invest in road, rail, sea or air infrastructure and vehicles.

Usually when using OpenTTD for recreation there would be one or more human players that each control a company. However, as mentioned at the beginning of Chapter \ref{chapter:introduction}, this is not a requirement: OpenTTD allows non-human players, so-called AI players, fully controlled by custom code supplied by the player/researcher. If there are no human players, as discussed later in the current chapter, OpenTTD also then runs deterministically---using a seed value on startup that controls all pseudo-random behavior of the game. These abilities are the core abilities that allows OpenTTD to be used in repeatable, reproducible and, hopefully, replicable simulations.

This chapter explores this model and these abilities in more detail, as well other related existing features that allow OpenTTD to be controlled programatically and ultimately used, through the OpenTTDLab framework presented in the next chapter, to run simulations. This chapter also shows that the environment of OpenTTD is a rich one; while there are clearly some unrealistic aspects, it is at least a tempting target for running simulations that could be used to extract  insights about the real world.

Unless otherwise stated, the details here are from OpenTTD Wiki \cite{OpenTTDWiki}, the OpenTTD source code \cite{OpenTTDSource}, the OpenGFX source code \cite{OpenGFXSource}, the OpenTTD AI API documentation \cite{OpenTTDAIAPIDocs}, the OpenTTD GameScript API documentation \cite{OpenTTDGSAPIDocs}, or by playing OpenTTD itself. Details are correct as of OpenTTD 13.4.

\section{Field of play}

OpenTTD runs on two-dimensional rectangular tiled grid representing an area of land and sea, with some limited three-dimensional aspects. The grid size is configurable on startup between 64x64 and 4096x4096 tiles. Each tile is square, albeit presented to the user in a dimetric view as can be seen in Figure \ref{figure:introduction-screenshot}\footnote{A \emph{dimetric} view is sometimes inaccurately referred to as an \emph{isometric} view.}. Each tile does not necessarily have a single object on it, some objects can co-exist with each other depending on the object type. The positions of vehicles that carry goods and passengers, which are the main mechanism for making money in the game, are even more granular---they move and take positions at what appear to be any position on their path between destinations.

There is some concept of height: each section of a grid can have one of a limited number of height levels in the game. This impacts the game in more than just a visual way: when going uphill road and rail vehicles usually slow down, and as discussed later, this can have an impact on money made. Rail and road tunnels can be built by the companies which can avoid this, but they have their downsides: for example they typically cost the company more to build, and can have their own speed restrictions.

An example field of play, also showing some of the interface that players use to interact with the field of play is show in Figure \ref{figure:introduction-screenshot}.

\section{Startup}

There are two classes of startup: a pre-defined scenario can be started, where the map is pre-defined; and a pseudo-random world. In the pseudo-random case, which is the case of all experiments of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling}, the world is deterministically generated from  the configuration of the game and an integer seed to the built-in random number generator, referred to as the \emph{random seed}. Specifically, the land, height, trees, coastlines and sea, the sizes and positions of towns, and the positions and types of industries are all deterministically generated. This determinism means that OpenTTD can be used for bitwise repeatable experiments.

No transport infrastructure exists on startup other than roads within towns. It is the role of the players, including the AI players, to build these to connect towns and industries in order to earn revenue. On startup each company is provided a loan with which to begin building the infrastructure.

\section{Climates}

OpenTTD has four different \emph{climates}: \emph{temperate}, \emph{sub-arctic}, \emph{sub-tropical} and \emph{toyland}. A climate is fixed on game startup and effects a variety of properties of the game such as what transportation types are available, and what industries and supply chains are possible. For conciseness all descriptions are limited to the temperate climate, as are the experiments of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling}.

\section{Economic and transportation model}

The possible supply chains in the temperate climate of OpenTTD can be seen in Figure \ref{figure:temperate-supply-chains}. In total there are 13 different types of industries that can produce or receive cargo: iron ore mines, coal mines, forests, farms, steel mills, oil wells, power stations, sawmills, factories, oil refineries, oil rigs, banks and towns; and 11 types of cargo: iron ore, coal, wood, livestock, grain, steel, oil, goods, valuables, mail and passengers. Towns and passengers are maybe not technically industries and cargo, but I include them because in terms of the economic model of OpenTTD they are.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={rectangle,draw}]

    \node[main node, align=center,minimum size=0.7cm] (factory) {\footnotesize Factory};
    \node[main node, align=center,minimum size=0.7cm,on grid,right=3cm of factory] (town) {\footnotesize Town};
    \node[main node, align=center,minimum size=0.7cm,on grid,right=3cm of town] (oil-rig) {\footnotesize Oil rig};
    \node[main node, align=center,minimum size=0.7cm,on grid,left=3cm of factory] (steel-mill) {\footnotesize Steel mill};
    \node[main node, align=center,minimum size=0.7cm,on grid,left=3cm of steel-mill] (iron-ore-mine) {\footnotesize Iron ore mine};
    \node[main node, align=center,minimum size=0.7cm,on grid,below=1.5cm of steel-mill] (oil-wells) {\footnotesize Oil wells};
    \node[main node, align=center,minimum size=0.7cm,on grid,above=1.5cm of steel-mill] (farm) {\footnotesize Farm};
    \node[main node, align=center,minimum size=0.7cm,on grid,above=1.5cm of farm] (forest) {\footnotesize Forest};
    \node[main node, align=center,minimum size=0.7cm,on grid,below=3cm of oil-rig] (bank) {\footnotesize Bank};
    \node[main node, align=center,minimum size=0.7cm,on grid,below=1.5cm of oil-wells] (power-station) {\footnotesize Power station};
    \node[main node, align=center,minimum size=0.7cm,on grid,left=3cm of power-station] (coal-mine) {\footnotesize Coal mine};
    \node[main node, align=center,minimum size=0.7cm,on grid,right=3cm of forest] (sawmill) {\footnotesize Sawmill};
    \node[main node, align=center,minimum size=0.7cm,on grid,below=1.5cm of factory] (oil-refinery) {\footnotesize Oil refinery};

    \path[every node/.style={font=\scriptsize}]
        (forest) edge[] node[] {Wood} (sawmill)
        (bank) edge [loop above, distance=0.6cm] node[] {Valuables} (bank)
        (coal-mine) edge[] node[] {Coal} (power-station)
        (iron-ore-mine) edge[] node[] {Iron ore} (steel-mill)
        (oil-wells) edge[] node[] {Oil} (oil-refinery)
        (oil-rig) edge[loop above, distance=0.6cm] node[] {Passengers, mail} (oil-rig)
        (factory) edge[] node[] {} (town)
        (steel-mill) edge[] node[] {Steel} (factory)
        (town) edge[loop above, distance=0.6cm] node[] {Passengers, mail} (town);

    \path[draw,->] 
    (oil-rig.south)
    -- ($ (oil-rig.center) - (0,1.5) $)
    -- node[above] {\scriptsize Oil}
    (oil-refinery.east);

    \path[draw,<->] 
    (oil-rig.west)
    -- node[above] {\scriptsize Passengers} node[below] {\scriptsize Mail} 
    (town.east);

    \path[draw,->] 
    (farm.east)
    -- node[above] {\scriptsize Livestock} node[below] {\scriptsize Grain} ($ (farm.center) + (3,0) $)
    --
    (factory.north);

   \path[draw,->] 
    (oil-refinery.north)
    -- ($ (oil-refinery.center) + (0,0.75) $)
    -- ++(1,0)
    -- ++(0,0.55)
    -- ($ (town.west) - (0,0.2) $);

    \path[draw,->] 
    (sawmill.east)
    -- ($ (sawmill.center) + (1,0) $)
    -- ++(0,-2.8)
    -- node[above] {\scriptsize Goods} ($ (town.west) + (0,0.2) $);

\end{tikzpicture}
\caption{The possible supply chains of the temperate climate of OpenTTD 13.4, showing what types of cargo industries and towns accept and produce. Adapted from `Flow chart for Temperate Cargo' in the \emph{OpenTTD Wiki} \cite{TemperateFlowChart}.}
\label{figure:temperate-supply-chains}
\end{figure}

The companies in OpenTTD, controlled by either human or AI players, receive money for transporting the 11 cargo types between the 13 industry types. The amount of money is dependant on a number of
factors: the amount of cargo, the cargo type, and the time taken--a shorter time results in more money, and the distance between source and destination---longer distance results in more money. How much each
industry or town depends on on how much input cargo it has received, a hidden property of its total (pseudo-randomly chosen in pseudo-randomly generated worlds), which can also change during the game similarly pseudo randomly.

The 11 cargo types can be transported by 4 classes of transportation: road, rail, sea and air; and each of these have further sub classes. With the exception of oil rigs that have built-in stations, none of the industries have stations for the transportation to pick up or drop of cargo, and so companies must build them at a cost. Stations can also be only built on flat or almost flat land, which requires earth works which also costs. Companies must also build the road or rail networks to connect the stations. Road and rail networks cannot be built arbitrarily on sloping land: there are often earth works required to shape it which costs the company. This fact explains some of the results in Chapter \ref{chapter:experiments-attempt-at-reproducing}. Depots must also be built in order to then build the vehicles that can transport the caxrgo; with the exception of some airports. Once built, the company configures each vehicle with a route: a set of stations to travel between to pick up and drop off cargo. The minimum useful route would be 2 stations long, but they can be longer.

There are a number of obvious absurdities to the economic model. The number of good for example: there are 11 in OpenTTD, but the UK Tariff, the listing of all rules that govern imports into the UK, refers to over 25,000 different commodities \cite{uktariff}. Also, as long as an industry's type can accept that type of cargo, it can accept any amount of that cargo and will pay the company that transported it. It also doesn't matter which specific example of an industry any particular piece of cargo goes, which includes passengers; for example when a passenger arrives at a station, that particular passenger has no particular destination in mind in the game---the company will receive money for taking them anyway, but more money if they are taken further.

It is beyond the scope of this project to more rigorously analyse or validate the economic model of OpenTTD, and to so work out how much any insights generated by OpenTTD simulations can be applied to the real world. As briefly mentioned in the previous chapter, it has been suggested that some of OpenTTD's network effects, presumably involving the supply chains of Figure \ref{figure:temperate-supply-chains}, could be realistic. I hold out hope that OpenTTD could be used as a valid simulator for investigating network effects, and specifically by using OpenTTDLab presented in the next chapter.

\section{Agents, ticks, and threads}

OpenTTD is an agent-based system, with agents: buses, trains, planes, ships, industries and even buildings in towns to an extent running apparently independently and in accelerated real time. However, the real-time aspect is somewhat of an illusion. OpenTTD runs on the concept of \emph{ticks}, where there are 74 ticks per in-game day. During a tick, game time progresses in a deterministic way---vehicles move a certain amount, pick up or drop off a certain amount of passengers or goods, industries produce a certain amount of goods, a number of passengers are "produced" by towns wanting to travel.

This determinism makes OpenTTD particularly suitable for exactly repeatable simulations. If, for example, instead of ticks, threads were used for the various agents, since thread scheduling is in general non deterministic, then exactly repeatable experiments would not be possible. OpenTTD does use threads in secondary activities, such as showing progress bars or saving games, but with these exceptions the core of OpenTTD can be described as \emph{single-threaded}.

\section{OpenTTD AIs}

OpenTTD AIs are scripts, written using the Squirrel language, that can be plugged into OpenTTD at runtime that allows it to control a company. OpenTTD offers such AIs a rich API, with 545 functions to control the game or inquire into its state \cite{OpenTTDAIAPIDocs} where many of the functions act on specific tile(s) of the field of play. It works within the tick framework, in that every tick of the game only a certain amount of the AI script can progress. OpenTTD AIs can also be parameterised---accepting parameters set before the start of the game. For example, the AI created for the experiments of Chapter \ref{chapter:experiments-simple-parameterised-ai} accepts a single parameter that controls how many buses to create. For the avoidance of doubt, OpenTTD AIs can be extremely simplistic, without any advanced learning behaviour often associated with the term \emph{AI}.

This AI system is the core feature that allows OpenTTD to be used to experiment how different strategies of AIs affect the outcome, and so, to an extent, simulate properties of the real world. The framework described in Chapter \ref{chapter:openttdlab-design-process-and-features} was designed to make it straightfoward to source, configure, and run such AIs, and all of the experiments of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling} use them.

\section{Configurability}

OpenTTD has over 400 options that allows the game to be customised; it is not useful to review them all here, see \cite{OpenTTDWiki} for the complete list and their defaults. However, there are several that are particularly relevant to using OpenTTD to conduct repeatable, reproducible or replicable experiments.

\begin{itemize}
\begin{descitem}{Properties of the pseudo-randomly generated world}
As suggested earlier in the current chapter, OpenTTD allows the configuration of the dimensions of the world, between  64x64 and 4096x4096 tiles. It also allows the specification of other properties, such as if the generated world is flat or mountainous. Flat and mountainous worlds are compared in the experiments of Chapter \ref{chapter:experiments-attempt-at-reproducing}.
\end{descitem}
\begin{descitem}{Total number of ticks}
OpenTTD allows the specification of the maximum number of ticks taken before it exits. This allows experiments to run for a fixed amount of in-game time, rather than the default of indefinitely. All of the experiments of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling} do this.
\end{descitem}
\begin{descitem}{Random seed}
As described earlier in this chapter, OpenTTD allows the specification of random seed which controls how the world is generated at the start of play.

Keeping track of the random seeds allows experiments to be bitwise repeatable, and including the random seeds with published results should help them be (bitwise) reproducible, such as the experiments of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling}.
\end{descitem}

\end{itemize}

OpenTTD also requires what's known as a \emph{base graphics set}. While this does not affect gameplay, it affects the visuals of the game. OpenGFX \cite{OpenGFXSource} is an open source base graphics set that is easily installable through the OpenTTD interface.

It should be noted that while OpenTTD has a wide range of options, there is nothing in OpenTTD to help run it over a specific range of configurations, for example over two different configurations of worlds, or over a range of random seeds.

\section{Savegames}

OpenTTD 13.4 and earlier can be configured to automatically save the state of the game in intervals of in-game time in files of a custom binary format. These \emph{savegames} are designed to be able to pick the game for when they left off. However, they are also rich sources of information for analysis, containing for example the complete transportation network each company has constructed, and how much each company is worth or how much money it has in the bank. OpenTTD 14.0 no longer has deterministic saving, but there is a way to restore this ready for OpenTTD 15.0.\footnote{OpenTTD 14.0 changed how games were saved from intervals of in-game time to intervals of wallclock time, which makes the savegames non-deterministic, and so makes it virtually impossible to use them to extract information from OpenTTD for bitwise repeatable experiments. However, as part of this project I successfully submitted a change to OpenTTD that allows the scheduling of so-called \emph{console scripts} that can be used to restore the behaviuor of OpenTTD 13.4 \cite{OpenTTDScheduleScript}. This change is scheduled for release in OpenTTD 15.0.}

It is notable that no version of OpenTTD has a built-in ability in OpenTTD to extract detailed information from these savegames in a form suitable for analysis, or indeed aggregate such information over multiple savegames in order to see changes over in-game time. Also in spite of their deterministic nature and richness, so far there is no evidence of existing research using savegames to extract information from OpenTTD.

\section{Conclusion}

OpenTTD presents an extremely rich landscape for investigation; and because of the tick model, and the fact it accepts a random seed, this landscape is deterministic and so especially suitable for repeatable research. While its economic model is simplified or even absurd in parts, depending on the map size, there are hundreds of millions of possible actions an OpenTTD AI can take on any given tick of the game, and the existing research has only begun to explore this space.

If viewing OpenTTD not as a game for which it was designed, but as a tool for running repeatable, reproducible, and hopefully replicable, research, there are three core features that are not present in OpenTTD. Firstly, there is no way to configure OpenTTD so it runs over a range of available configurations, for example over a range of random seeds. And secondly while it saves information to its savegame files, there is no way to extract and combine this information into a form ready for analysis. And thirdly, while OpenTTD is largely single-threaded, which contributes to its determinism and bitwise repeatability \& reproducibiliy, it means there is no way to leverage multiple CPU cores. These are the core features that OpenTTDLab, as described in the next chapter, effectively add to OpenTTD.

\chapter{OpenTTDLab: Its design and features}
\label{chapter:openttdlab-design-process-and-features}

\begin{figure}[H]
\centering
\includesvg{assets/openttdlab-logo.svg}
\caption{The OpenTTDLab logo. I created this based on the existing OpenTTDLogo (licensed under GPLv2) \cite{OpenTTDLogo}, and it is displayed alongside the code of OpenTTDLab in its public documentation as can be seen in Appendix \ref{chapter:openttdlab-documentation}.}
\label{fig:openttlab-logo}
\end{figure}

As discussed in Chapter~\ref{chapter:introduction}, repeatability is the domain of the original researchers, while both replicability and reproducibility are the domain of other researchers armed with the original results. Given the different aims and audiences, it's not immediate that a single tool can be used in all of these. However, the Python framework OpenTTDLab I created attempts to be useful in all of these: specifically by adding the missing features described at the end of the previous chapter to help address at least some of 7 issues detailed in Chapter~\ref{chapter:introduction}. This chapter details the design process I followed to create OpenTTDLab to do this, and the resulting internal behaviour and user-facing features of the framework.

\section{Design and creation process}

I used a lightweight and highly-agile process to design and create OpenTTDLab, the high level features of which are shown Figure \ref{fig:solo-agile}. After I identified the problems to address, as discussed in Chapter~\ref{chapter:introduction}, I conducted many cycles of writing usage documentation for code, writing code, and using the code, either to construct the results of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling} or in tests; and each of these with tight cycles of evaluating the outputs of these activities and informally judging if the code does (or would if it existed) help produce research that was repeatable, replicable and reproducible. Each of these activities would inform each other in tight cycles.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick,main node/.style={circle,draw}]

    % https://tex.stackexchange.com/a/102266
    \tikzset{
        position/.style args={#1:#2 from #3}{
            at=(#3.#1), anchor=#1+180, shift=(#1:#2)
        }
    }

    \node[main node, align=center] (reqs) {Identify\\[-2mm]problems};
    \node[main node, align=center,minimum size=2.5cm] (code) [position=90:1cm from reqs] {Document\\[-2mm]code};
    \node[main node, align=center,minimum size=2.5cm] (docs) [position=-148:1cm from reqs] {Write\\[-2mm]code};
    \node[main node, align=center,minimum size=2.5cm] (expe) [position=-32:1cm from reqs] {Use\\[-2mm]code};

    \path[every node/.style={font=\small}]
        (reqs) edge [] node[] {} (code)
               edge [] node[] {} (expe)
               edge [] node[] {} (docs)
        (docs) edge [in=-150,out=-120,distance=1cm,loop] node[] {Evaluate} (docs)
               edge [bend left,arrows=<->] node[] {} (code)
               edge [bend right] node[left] {} (expe)
        (code) edge [distance=1cm,loop above] node[yshift=0.1cm] {Evaluate} (code)
               edge [bend left,arrows=<->] node[] {} (expe)
        (expe) edge [in=-60,out=-30,distance=1cm,loop] node {Evaluate} (expe)
               edge [bend left,arrows=<->] node[] {} (docs)
               edge [bend right,arrows=<->] node[] {} (code);
\end{tikzpicture}
\caption{The process I followed designing and creating OpenTTDLab. After identifying the problems that I'm trying to address, I conducted cycles of documenting code (that may not have existed), writing the code, and using the code, all coupled with tight cycles of evaluating how it solved, or would solve, the problem of running experiments using OpenTTD that were repeatable, replicable and reproducible.}
\label{fig:solo-agile}
\end{figure}

I use the term lightweight in that there were no \emph{events} that are common to \emph{Scrum}-like processes \cite{SCRUM}\footnote{From my own experience Scrum events are often known as \emph{ceremonies}}, and with one exception, no phases planned in advance as is often prescribed, for example for UK government services \cite{GOVUKAgile}. And I describe the process as highly-agile because in some cases iterations would take a low number of seconds, especially when writing documentation before the code it documents existed.

This was in fact the one exception to not having phases planned in advance: I did plan to create an initial version of light usage documentation very early in the project. Taking inspiration from Tom Preston Warner's Readme driven development \cite{ReadmeDrivenDevelopment} and Amazon's Working Backwards method \cite{bryar2021working}, throughout the project but especially at the beginning, I would write documentation for code without the code actually existing. This allowed me to imagine it being used, evaluate how well it would solve the problems I'm trying to solve, and change its design accordingly by quickly changing its documentation. Of course, documentation for code that does not exist has very little value, so I moved quickly to writing and using code, mostly informed by "Working software is the primary measure of progress" and "Deliver working software frequently" from the Agile Manifesto \cite{beck2001manifesto}, which I judge to be the most useful parts of the Agile Manifesto, especially when working alone, albeit with advice from others; the other parts are less applicable---they focus on communication between different people involved in project.

I did not attempt to keep a precise record of the cycles. However, as a guide, the GitHub repository that stores the history of code changes can be used \cite{OpenTTDLab}. I created 72 releases, which I labelled v0.0.1 through v0.0.72, 215 pull requests (PRs), and over 250 non-merge commits in the repository, and each of these I evaluated against problems and desired properties of Chapter~\ref{chapter:introduction}, and then carried on with more of the same activity, or another activity of Figure \ref{fig:solo-agile}.

The first version, v0.0.1, was the result of a fork from Patric Stout's \emph{OpenTTD-savegame-reader} \cite{Stout2024}, an existing Python project that extracts data from OpenTTD savegame files---as described in Chapter~\ref{chapter:openttd-model-and-abilities} these are files that save the state of play in OpenTTD that allows players to exit but resume at the same point later. This project could be be used to extract data from OpenTTD, and seemed a reasonably place to start; but, as mentioned at the end of the same chapter, even with this code there was still no way to easily run OpenTTD over a range of configurations, or extract information from the savegame files in bulk, and no way to leverage multiple CPU cores.

Then, but still from very early in the process, v0.0.3, I created and maintained a small suite of tests for OpenTTDLab, asserting on its high level behaviour. However, the tests were not just for asserting on the behaviour of the code, they were also a special case of ``Use code'' in the design process of Figure~\ref{fig:solo-agile}: they allowed me to extremely quickly gauge the suitability of the design of the OpenTTDLab, because to write these tests I used OpenTTDLab in a way very similar to those that experimenters would use it. I also configured these test to run automatically on every change: a ```very helpful'' property for software used in reproducible research according to \emph{The Turing Way}~\cite{turingway2022}. The tests essentially use OpenTTDLab to run experiments and assert on their results; they must be identical to previous runs of the experiments to pass, and so themselves give evidence that OpenTTDLab can be used to run bitwise repeatable experiments.

This leads to the main downside of this process: while the process allowed a high number of cycles of evaluations, all evaluation was from myself, from either running the code or, even worse, just imagining running the code. Being the creator I knew it extremely well and so was ill-placed to evaluate what amounts to its ease of use for people that don't know it as well: I'm essentially \emph{marking my own homework}, and in some cases before it was even written. This is true when writing any code, but in this case this is especially true as are different groups of people that are the target audience: original researchers, reproducers, and replicators. User research was not included in the process in order to limit the scope of the project, and to allow as many iterations as possible within the available time. However, while making OpenTTDLab I conducted (initial versions of) the three different experiments of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling} in which I played multiple roles: I conducted original research and used OpenTTDLab to attempt to reproduce existing research, and so slightly mitigating this downside.

\section{Python and portability}

% Through the design and creation process, particularly after evaluation, I made a number of high level architectural decisions: the use of Python, behaving identically where possible on different platforms, automatically downloading OpenTTD and OpenGFX, supporting both AIs in the local filesystem, and automatically downloading AIs, caching, and the use of parallelisation.

I chose Python as the main language for both the internals of OpenTTDLab and its user-facing interface. I knew Python well, as mentioned earlier in this chapter a savegame parser for OpenTTD was available in Python \cite{Stout2024}, programs written in Python are mostly platform independent, and anecdotally it's a popular language in data science and data analysis---data analysis and visualisation can be performed in Python through libraries such as Pandas \cite{mckinney-proc-scipy-2010,reback2020pandas,} and Plotly \cite{plotly}. To make sure OpenTTDLab did not depend on specific behaviour of a single Python version or a single platform, which would impede reproducibility of experiments especially \cite{turingway2022}, I made OpenTTDLab \emph{portable}--hiding platform-specific details, and making sure that the tests mentioned in the previous section, than ran on every change of OpenTTDLab, run and pass on multiple version of Python, but also on Linux, Windows and macOS. While OpenTTD itself is classed as portable, downloading, uncompressing, configuring, and starting OpenTTD are slightly different on each platform, so running tests on each is particularly valuable.

\section{Feature 1: Running over a range of configurations}

OpenTTDLab can run a specific version of OpenTTD and OpenGFX over a range of configurations: a range of random seeds and range of other configurations, and a range of OpenTTD AIs and their parameters with the numbers of runs made clear. This is through a single Python statement, with no steps ahead of time needed other than the installation of Python and OpenTTDLab. This requires that under the hood OpenTTDLab downloads, and caches, OpenTTD, OpenGFX, and OpenTTD AIs and libraries they depend on published on GitHub or OpenTTD's TCP-based BaNaNaS [sic] content publishing service \cite{OpenTTDBaNaNaS}, or on the local filesystem. All files downloaded from OpenTTD servers are cached at the request of Patric Stout, a core OpenTTD maintainer. An example use of OpenTTDLab can be seen in Listing \ref{listing:openttdlab}.

\begin{lstlisting}[label=listing:openttdlab,language=Python, caption={Example usage of OpenTTDLab that runs the trAIns AI automatically downloaded from BaNaNaS for a single year of in-game time for random seeds 0 to 9.}]
from openttdlab import run_experiments, bananas_ai

results = run_experiments(
    openttd_version='13.4',
    opengfx_version='7.1',
    experiments=(
       {
         'seed': seed,
         'ais': (
             bananas_ai('54524149', 'trAIns', ai_params=()),
         ),
         'days': 365 * 4 + 1,
       }
       for seed in range(0, 10)
    ),
)
\end{lstlisting}
%
Keeping to one line of Python code and so with no steps required ahead of time I found extremely convenient for repeatability, and I argue is important for reproducibility because it means only Python code can be shared as artifacts, with no additional information required other than the version of Python and OpenTTDLab; although as discussed the version of Python should not affect the results.

\section{Feature 2: Output as lists and dictionaries}

As noted one of the missing features of OpenTTD, if viewing it as a simulator rather than a game, was the fact that data output was in a series of binary savegame files of a custom format: not in a format immediately suitable for analysis. I decided to convert all the savegame files from all the runs of OpenTTD into a single self-descriptive data structure: a Python list of dictionaries. 

The this means that OpenTTDLab is agnostic to the specific way of how a user would prefer to analyse output; or in the language of Chapter~\ref{chapter:introduction} it is reusable in this respect. But as can be seen in Listing~\ref{listing:openttdlab-analyse}, only a statement is required to convert a subset of the output into a Pandas \emph{data frame}, a common first step when using Pandas to analyse data.

\begin{lstlisting}[label=listing:openttdlab-analyse,language=Python, caption={Example usage of the results of OpenTTDLab, such as those generated in \ref{listing:openttdlab}, converting them to a Pandas DataFrame.}]
import pandas as pd

df = pd.DataFrame(
    {
        'seed': row['experiment']['seed'],
        'date': row['date'],
        'money': row['chunks']['PLYR']['0']['money'],
    }
    for row in results
)
\end{lstlisting}

An alternative would be to output Pandas data frame(s) directly from OpenTTDLab. However, the main downside of this approach is that I would have to design a structure for the data frame(s), but since OpenTTD savegame data is hierarchical there is no unique or one-size-fits-all approach to convert such data into two such data frames: any approach would have pros and cons, and to investigate these I decided was out of scope of the current work. For similar reasons I rejected saving the results to non-ephemeral file such as SQLite: a tempting target since SQLite is a preferred format by the US Library of Congress. How to save data non-ephemerally is left to client code; for example a simple CSV format can be chosen to save a subset of the output, which is the approach I chose to persist the results of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling}.

Early on in the design process, in v0.0.2, I considered structuring both the API and the output to have a \emph{verification} step to make it easier to confirm that both configuration and results were bitwise identical to previous experiments. While this might have been particularly useful to use OpenTTDLab for knowing if results have been bitwise reproduced, it made generating the results of Chapters  particularl awkward---it seemed like this would reusability of OpenTTDLab. For this reason I abandoned this approach.


\section{Feature 3: Leveraging multiple CPU cores}

OpenTTDLab leverages multiple CPU cores by implementing simple task-based parallelism: it starts a process per experiment to run OpenTTD and to extract data from its savegame files and then aggregate all the results back to the starting process. By default the number of processes is the number of CPU cores on the system.  To further reduce time, a mechanism to reduce the amount of data copied between processes is offered by OpenTTDLab. These features was added to OpenTTDLab to help repeatability: specifically to run more repetitions of experiments in a given amount of time.

On a more emotional note, I felt frustrated by waiting for experiments to run, and this frustration encouraged me to reduce the number of repetitions rather than increase them. I also suspected that others using OpenTTDLab would feel similar frustration, and result in them repeating experiments less rather than more.

\section{Documentation}

As discussed, writing documentation was part of the design process. However, since documentation is especially important for reusable software used for reproducible research \cite{turingway2022}, so this documentation I repeatedly extended and is published and made available with OpenTTDLab, as it can be seen in Appendix \ref{chapter:openttdlab-documentation}. In addition to usage instructions, examples, and API documentation, it includes recommendations relating to the repeatability, reproducibilty and replicability of work that uses OpenTTDLab; replicability is addressed by encouraging authors to include details of the algorithms used in OpenTTD AIs created as part of the work.

As part of this process, I made a logo for OpenTTDLab as can be seen in Figure \ref{fig:openttlab-logo} based on the OpenTTD logo. I did not have a rigorous justification for this, but I suspected that creating such an identity for OpenTTDLab, and especially one that was related to the identity of OpenTTD itself, would encourage the reuse of the OpenTTDLab, and even encourage contributions to it.

% \section{Algorithm}

% \begin{algorithm}
% \caption{The core algorithm of OpenTTDLab}\label{alg:openttd}
% \KwData{The list of experiments to run, where each is defined by the seed, number of days, list of AIs and parameters to pass to the AIs. Versions of OpenTTD and OpenGFX}
% \KwResult{The parsed save games from every month of game time for every experiment} 
%  fetch everything that needs to be fetched (not stuff that is already in cache...)\;
%  setup threaded pool for workers\;
%  \ForEach(in parallel up to {max\_workers}){experiment in experiments}{
%   run experiment by running OpenTTD\;
%   parsing all the save games\;
%  }
% \end{algorithm}

\section{The 7 issues of existing OpenTTD research}

The three features of OpenTTDLab address 6 of the 7 issues in existing OpenTTD research described in Chapter~\ref{chapter:introduction}. Because of the low amount of Python code needed to run experiments and analyse results, it should be straightforward to include these artifacts in published work, or at least make the code available separately and link to it (Issue 1). OpenTTDLab leverages multiple cores and so supports running more repetitions in a given amount of time, and sharing the Python artifacts shares how many repetitions were run (Issue 2). As can be seen from Listing \ref{listing:openttdlab}, it is straightforward to run multiple experiments  over the exact same amount of in-game time (Issue 3). There are no manual steps involved to download, configure, or run OpenTTD or extract data (Issue 4). The versions of OpenTTD, OpenGFX, and details of configuration are included in the artifacts (Issue 5), which also includes the random seeds used (Issue 6).

The issue of replicating results (Issue 7) is not directly addressed by features of OpenTTDLab. However, this is still addressed in two ways: as mentioned in the previous section, its documentation encourages authors that use OpenTTDLab to include explicit details of the algorithms created; and by the transitive nature of the 3Rs described in the introduction, helping repeatability and reproducibility indirectly aids replicability.

\section{Conclusion}

OpenTTDLab offers three features through a single Python statement: it allows a researcher to easily run OpenTTD over a range of configurations; it returns results from these experiments in a way that's suitable for analysis; and it leverages multiple CPU cores which allows more experiments to be run in a given amount of time. These are all useful for repeating experiments; and if artifacts are shared, then reproducing experiments; and if the algorithm of any OpenTTD AIs created is shared as encouraged by the OpenTTDLab documentation, also replicating experiments.

In the following chapters I give more direct evidence of OpenTTDLab's usefulness in repeating and replicating by using OpenTTDLab in experiments. In Chapter~\ref{chapter:experiments-attempt-at-reproducing} I play the role of replicator, using OpenTTDLab to attempt to replicate existing research. In Chapter~\ref{chapter:experiments-simple-parameterised-ai} I play the role of original researcher, creating a simple OpenTTD AI; also suggesting that OpenTTD is a compelling landscape for simulation and worthy of further research. In Chapter~\ref{chapter:experiments-scaling} I provide evidence that the parallelism features added actually work, and argue that this helps repeatability.

% \chapter{Example results}

% To get validate of OpenTTDLab's usefulness in terms of repeatable, reproducible and replicable experiments, I used OpenTTD in 3 different situations. Firstly, to attempt to reproduce existing results from authors of an existing OpenTTD AI, and even in one respect go further in terms of showing what results are possible to extract and analyse. Secondly, to explore how a single parameter can change how a new simple AI performs, and shows that OpenTTDLab can be used to explore risk-benefit trade offs in basic supply chains. And finally to show how OpenTTDLab can be used to programmatically optimize this parameter using a basic algorithm.

\chapter{Experiments 1: Reproducing results}
\label{chapter:experiments-attempt-at-reproducing}

To attempt to validate OpenTTDLab as a useful framework, I used it to attempt to reproduce some of the results of Rios and Chaimowicz \cite{rios2009trains}: a set of experiments running two AIs, trAIns [sic] being presented in the aforementioned work and the pre-existing AdmiralAI, and comparing their company values. I generated results, and while trAIns does appear to perform better than AdmiralAI, the performance of trAIns does not match the original work.

\section{Experimental setup}

The setup was a best effort attempt to what was described as the setup of the experiments in the original work: the starting year was set to 1960, the map was configured to be 512x512 tiles with a very low amount of seas, industry density and number of towns set to normal, the economy set to be able to fluctuate, subsidy multiplier of x2, disasters enabled, tolerance of town councils to changes set as tolerant, both vehicle running costs and construction costs set to medium, vehicle breakdowns disabled, trains set to only be able to turn around at the end of the line as opposed to also at stations, and the maximum initial loan was set to £300,000, and initial interest rate set to 3\%.

Note that these settings were not all explicitly mentioned in the original paper: it described running OpenTTD revision 16724 using \emph{medium} difficulty level with some overrides. However, OpenTTDLab does not work under this version (due to savegame format differences), and the recent versions of OpenTTD that do work under OpenTTDLab no longer have the concept of difficulty level. However, the historical meaning of the medium difficulty level is documented \cite{OpenTTDDifficultyLevels}, and so the explicit list of aforementioned settings recreated, and run under OpenTTD 13.4 and OpenGFX 7.1 that OpenTTDLab does work under. There is one exception to basing off the medium difficulty level: the medium difficulty level was documented to have a maximum initial loan of £150,000, but using this level in pilot experiments resulted in extremely poor performance of AdmiralAI---it appeared to be virtually unable to function.

None of the versions of the AIs or their dependencies were specified in the original paper. In the current experiments, the version of trAIns is 2.1,  the version of AdmiralAI is 25, and the version of AdmiralAI's single dependency, Queue.FibonacciHeap, is 2. All the AIs and their dependencies these were retrieved from BaNaNaS.

The results presented here are for exactly 15 years of in-game time, while in the original work the time varied from 15 years and 2 days to 15 years and 11 months. In both the original paper and here, there were two sets of experiments performed: one using the \emph{flat} terrain type, and one for the \emph{mountainous} terrain type. Here it is repeated 64 times, and in the original paper each of these was repeated 7 times. Here random seeds 0 to 64 were used for each set of experiments, but in the original paper the random seeds are unknown.

The experiments here were run on mac OS 14.5 running on a 2020 M1 and Python 3.11.0 and OpenTTDLab 0.0.72. The OS and CPU of the original experiments are unknown. The experiments here were run with OpenTTDLab's default number of workers, the number of cores on the machine, which in this case were 8.

The Python code used to run the experiments is in Appendix \ref{chapter:trains-vs-admiral-run-experiments}, and the code to subsequently analyse the results in Appendix \ref{chapter:trains-vs-admiral-analyse-results}.

\section{Results}

The results of the experiments can be seen in Figure \ref{figure:trains-ai-vs-admiral-ai-final-distributions}, showing the final distributions of company value for the two sets of experiments for both trAIns and AdmiralAI, comparing the results using OpenTTDLab to the results of the original paper; and in Figure \ref{figure:trains-ai-vs-admiral-ai-over-time}, showing how the distribution of company value changes over time for the two sets of experiments and two AIs, but not comparing them with the original paper because it did not include sufficient data.

From \ref{figure:trains-ai-vs-admiral-ai-final-distributions} it is clear that while trAIns performed better than AdmiralAI in both the OpenTTDLab and original experiments, the difference between the two AIs in the current experiments is slight, as opposed to the original paper when the difference was remarkable. While AdmiralAI's performance is similar or slightly worse than what was originally reported, especially in the mountainous configuration, trAIns can be characterised as approximately an order of magnitude worse. It can also be seen that in the current experiments there are more cases that can be classed as catastrophic---in the sense that the company value remains approximately zero---across both trAIns and AdmiralAI.

The results over time of Figure \ref{figure:trains-ai-vs-admiral-ai-over-time} show that for both trAIns and AdmiralAI the distribution of company value becomes increasingly wider with time. They also show that while the median value for trAIns is slightly higher than for AdmiralAI, the distribution of company value for trAIns is much wider than that for AdmiralAI, especially on flat terrain.

From both Figures \ref{figure:trains-ai-vs-admiral-ai-final-distributions} \& \ref{figure:trains-ai-vs-admiral-ai-over-time} it can be seen that in both the original and current experiments, both AIs appeared to perform better on flat terrain than on mountainous.

\begin{figure}[p]
\centering
\begin{gnuplot}[terminal=cairolatex,terminaloptions={size 5.5,3}]
file = "notebooks/01_trains_ai_vs_admiral_ai_results_02_combined_final.csv"
set datafile separator ","
set grid ytics
set format y "\\footnotesize %.0s%c"
set style data boxplot 
set style boxplot outliers pointtype 1 medianlinewidth 2
set style boxplot fraction 1.00
set ylabel '\footnotesize Company value $\textrm{\pounds}$'
set key right top

mylabel(name) = sprintf("%s", name)
set style fill pattern 2 border lt -1

set multiplot layout 1,2
set yrange [-2500000:50000000]
set ytic 5000000

set xlabel ""
set lmargin at screen 0.11; set rmargin at screen 0.53
set xtics ("\\footnotesize trAIns" 1.5, "\\footnotesize AdmiralAI" 3.5)
set label 1 '\footnotesize Flat' at graph 0.03,1.05
plot file \ 
   using (1):(strcol(6) eq "Original paper" && strcol(4) eq "Flat" && strcol(2) eq "trAIns AI" ? $3 : NaN):(0.5) lc rgb '#aaaaaa' fs pattern 2 notitle, \
   '' using (2):(strcol(6) eq "OpenTTDLab" && strcol(4) eq "Flat" && strcol(2) eq "trAIns AI" ? $3 : NaN):(0.5) lc 2 fs pattern 0 notitle, \
   '' using (3):(strcol(6) eq "Original paper" && strcol(4) eq "Flat" && strcol(2) eq "Admiral AI" ? $3 : NaN):(0.5) lc rgb '#aaaaaa' fs pattern 2 notitle, \
   '' using (4):(strcol(6) eq "OpenTTDLab" && strcol(4) eq "Flat" && strcol(2) eq "Admiral AI" ? $3 : NaN):(0.5) lc 2 fs pattern 0 notitle

set xlabel ""
unset ylabel
set format y ''
set lmargin at screen 0.53; set rmargin at screen 0.95
set label 1 '\footnotesize Mountainous' at graph 0.03,1.05
plot file \ 
   using (1):(strcol(6) eq "Original paper" && strcol(4) eq "Mountainous" && strcol(2) eq "trAIns AI" ? $3 : NaN):(0.5) pointsize 0.5 lc rgb '#aaaaaa' notitle, \
   '' using (2):(strcol(6) eq "OpenTTDLab" && strcol(4) eq "Mountainous" && strcol(2) eq "trAIns AI" ? $3 : NaN):(0.5) pointsize 0.5 lc 2 fs pattern 0 notitle, \
   '' using (3):(strcol(6) eq "Original paper" && strcol(4) eq "Mountainous" && strcol(2) eq "Admiral AI" ? $3 : NaN):(0.5) pointsize 0.5 lc rgb '#aaaaaa' fs pattern 2 notitle, \
   '' using (4):(strcol(6) eq "OpenTTDLab" && strcol(4) eq "Mountainous" && strcol(2) eq "Admiral AI" ? $3 : NaN):(0.5) pointsize 0.5 lc 2 fs pattern 0 notitle, \
   keyentry with boxplot lc rgb '#aaaaaa' fs pattern 2 title "\\footnotesize \\vspace*{0.1cm} Original paper", \
   keyentry with boxplot lc 2 fs pattern 0 title "\\footnotesize \\vspace*{0.1cm}OpenTTDLab"
\end{gnuplot}
\caption{Distributions of company values for trAIns and AdmiralAI at the end of experiments on flat and mountainous terrain, comparing the results as reported by the original work of Rios and Chaimowicz \cite{rios2009trains}, and those from experiments run with OpenTTDLab configured to attempt to replicate the original work. The boxes show the median and upper and lower quartiles, and the whiskers show the maxima and minima.}
\label{figure:trains-ai-vs-admiral-ai-final-distributions}
\end{figure}

\begin{figure}[p]
\centering
\begin{gnuplot}[terminal=cairolatex,terminaloptions={size 5.3,3}]
set datafile separator ","
set style fill pattern 2
set grid ytics
set format y "%.1s%c"
set timefmt "%Y-%m-%d"
set xdata time
set xtics "1960-01-01",3600*24*365*5,"1974-12-01"

set ylabel '\footnotesize  Company value $\textrm{\pounds}$' offset 1.5

set xrange ["1960-01-01":"1975-06-01"]
set yrange [-200000:4000000]

set multiplot layout 1,2

set ytics ("\\footnotesize 0" 0, "\\footnotesize 0.5M" 500000, "\\footnotesize 1.0M" 1000000, "\\footnotesize 1.5M" 1500000, "\\footnotesize 2.0M" 2000000, "\\footnotesize 2.5M" 2500000, "\\footnotesize 3.0M" 3000000, "\\footnotesize 3.5M" 3500000, "\\footnotesize 4.0M" 4000000)
set mytics 5

unset key
set lmargin at screen 0.11; set rmargin at screen 0.53
set xlabel '\footnotesize Date'
set format x "\\footnotesize %Y"

set label 1 '\footnotesize Flat' at graph 0.03,1.05
plot 'notebooks/01_trains_ai_vs_admiral_ai_results_03_openttdlab_company_value_quartiles.csv' \ 
   using (timecolumn(1, '%Y-%m-%d')):2:4 skip 4 with filledcurves lc rgb '#dddddd' fillstyle solid title '\footnotesize AdmiralAI Q1-Q3', \
   '' using (timecolumn(1, '%Y-%m-%d')):3 every 6 skip 4 with points pt 5 lw 8 lc 2 ps 0.4 title '\footnotesize AdmiralAI median', \
   '' using (timecolumn(1, '%Y-%m-%d')):6:5:7 every 6 skip 4 with yerrorbars lc 1 ps 0 title '\footnotesize trAIns Q1-Q3', \
   '' using (timecolumn(1, '%Y-%m-%d')):6 every 6 skip 4 with points lc 1 pt 7 ps 0.7 title '\footnotesize trAIns median'

set lmargin at screen 0.53; set rmargin at screen 0.95

set key right top
set key invert
set label 1 '\footnotesize Mountainous' at graph 0.03,1.05
set ylabel ''
set ytics ("" 0, "" 500000, "" 1000000, "" 1500000, "" 2000000, "" 2500000, "" 3000000, "" 3500000, "" 4000000)
set xtics "1960-01-01",3600*24*365*5,"1975-01-01"

plot 'notebooks/01_trains_ai_vs_admiral_ai_results_03_openttdlab_company_value_quartiles.csv' \ 
   using (timecolumn(1, '%Y-%m-%d')):8:10 skip 4 with filledcurves lc rgb '#dddddd' fillstyle solid title '\footnotesize AdmiralAI Q1-Q3', \
   '' using (timecolumn(1, '%Y-%m-%d')):9 every 6 skip 4 with points pt 5 lw 8 lc 2 ps 0.4 title '\footnotesize AdmiralAI median', \
   '' using (timecolumn(1, '%Y-%m-%d')):12:11:13 every 6 skip 4 with yerrorbars lc 1 ps 0 title '\footnotesize trAIns Q1-Q3', \
   '' using (timecolumn(1, '%Y-%m-%d')):12 every 6 skip 4 with points lc 1 pt 7 ps 0.7 title '\footnotesize trAIns median'

\end{gnuplot}
\caption{How the distributions of company value for trAIns and AdmiralAI change over time for experiments on flat and mountainous terrain using OpenTTLab, configured to attempt to replicate the results of Rios and Chaimowicz \cite{rios2009trains}.}
\label{figure:trains-ai-vs-admiral-ai-over-time}
\end{figure}

% \section{Replicating results}
% \lstset{numbers=left,frame=tb,basicstyle=\linespread{0.7}\ttfamily\footnotesize}
% \lstinputlisting[language=Python,float,caption=A floating example]{assets/replicating_results_core.py}

\section{Discussion}

The only aspect of the results that can be easily explained is the experiments on mountainous terrain resulted in lower company values than those on flat terrain. As discussed in Chapter \ref{chapter:openttd-model-and-abilities}, railways on mountainous terrain require more investment, for example by raising and lowering land or constructing more rail to go around obstacles, and there is nothing in the economic model of OpenTTD to offset this by subsequent high income. In fact it is the opposite: trains would take longer to travel between stations, due to either trains go slower when going uphill or round corners, which increase the time to transfer goods and so reduces income compared to an equivalent route but over flat terrain.

The other aspects of the results are currently unexplained, but I speculate reasons for some of them. For example, the distributions of company value getting wider as time goes on, assuming this is not just due to a random walk, the increasing distributions hint at a feedback loop where early strong performance increases the chance of later even stronger performance, and similarly for weaker performance. The only differences between the games in an experiment is the random seed that which effects world generation, if this is the case this in turn would suggest that something in initially generated world results in the better performance. Further data analysis of the results of the same experiments should show if there is a correlation between early and late performance. This is not included here for brevity, but it should be straightforward to re-run the experiments to further analyse the data. If such a correlation is found, this also hints at an evidence-based route for further improving the AI: beyond simply flat or mountainaous, there could be other properties of the generated worlds that lend themselves to better performance, and some worse. Finding out exactly what those are could lead to better AIs. trAIns AI appears to use fairly short term/local knowledge, rather than aggregate information about the map---potentially improvements could be made to use whatever insights are gathered statically after the facy more dynamically during the simulation. Although as discussed extract details of the world is not currently a feature of OpenTTDLab.

In terms of the seemingly increased prevalence of catastrophic results, it is has been suggested that one of the barriers to reproducibility of simulations is the under reporting of errors \cite{luijken2024replicability}; indeed, there are no mention of errors in the original study. It could also be that the number of repetitions in the original study are much lower, 7 per configuration rather than 64, and so just by chance resulted in better results than found here; although this does seem unlikely given the distributions of results between the original and current work in most cases did not overlap. Unfortunately, because the random seeds were not included in the original study, the possibility of \emph{seed hacking}\singlespacedfootnote{Seed hacking in this context would be an example of p-hacking or data dredging.} cannot be excluded from the list of possibilities for the reasons of the different results.

However, the most likely cause of differences are not deliberate, but in fact due to the differences in version of OpenTTD, its configuration, and the AIs. If this is indeed the case, then the results here suggest that trainsAI is less robust than Admiral to whatever differences these introduced.

\section{Conclusion}

It is beyond the scope of the current work to go beyond speculation and fully explain the results. However, these experiments have shown two things. Firstly, they give stronger evidence to the claims of Chapter \ref{chapter:introduction} that existing research that uses OpenTTD cannot be replicated. Secondly that OpenTTDLab can be used to extract rich information from OpenTTD, richer than that extracted than that in existing OpenTTD research; and as can be seen in Appendices \ref{chapter:trains-vs-admiral-run-experiments} \& \ref{chapter:trains-vs-admiral-analyse-results}, with a small amount of code that should be bitwise reproducible, and so could not just confirm the results, but further them through more in-depth analysis without even the full results being shared. The results here were also for a single metric: company value. However, as discussed briefly in Chapter \ref{chapter:openttdlab-design-process-and-features}, there is a wealth of further information available in OpenTTDLab.

\chapter{Experiments 2: Simulating a network}
\label{chapter:experiments-simple-parameterised-ai}

The previous chapter showed that OpenTTDLab makes it straightforward to compare existing AIs, and its richness of output allows informed speculation as to the reasons for their behaviour differences, and all in what I claim to be a bitwise repeatable and reproducible way. However, this doesn't give much evidence that OpenTTD, run using OpenTTDLab, can be used a a simulator to explore real world effects. In this chapter I present results of running a simple parameterised AI designed to explore the effects of that parameter changing. I find that, just informally or intuitively, OpenTTD shows some effects that align with intuitive expectations of the real world, and suggest that further work to validate OpenTTD as a simulator would be valuable. And by using OpenTTDLab the results are repeatable, by providing the code of the AI the results should be replicable, and I argue that by providing a high level algorithm for this AI, the results should be replicable.

\section{Experimental setup}

I constructed an OpenTTD AI specifically for the purposes of this experiment. At the start of the game, the AI chooses the 2 largest towns, find a route between them using the A* algorithm, builds a road over this route, a depot and station at each end, and builds a configurable number of buses that carry between the two stations. The algorithm in more detail can be seen in Algorithm \ref{algorithm:simple-parameterised-ai}, with the Squirrel code of the AI in Appendices \ref{chapter:simulating-a-network-squirrel-info} \& \ref{chapter:simulating-a-network-squirrel-main}, with a basic Python regression test that uses OpenTTDLab in Appendix \ref{chapter:simulating-a-network-python-regression-test}.

\begin{algorithm}
\caption{Simple parameterised OpenTTD AI}
\label{algorithm:simple-parameterised-ai}
\KwData {Random seed, the number of buses to build in the AI N}
 (OpenTTD generates the map deterministingly based on the random seed) \;
 Obtain the list of towns \;
 Sort the list of towns by size \;
 Choose 2 biggest towns \;
 \While{not found route}{
  Continue to find route \;
 }
  \ForEach{step in route}{
  build step\;
 }
 build a depot at each end of the route\;
 build a station at each end of the route;
 \ForEach{bus in buses}{
  build bus\;
  give the bus orders to move between the 2 towns\;
 }
\end{algorithm}

The number of buses varied between 1 and 16, and each run was for 50 years, and each confuration run for each integer random seed between 0 and 50 inclusive.

The experiments here were run on mac OS 14.5 running on a 2020 M1 and Python
3.11.0 and OpenTTDLab 0.0.72 and OpenTTD 13.4. The experiments here were run with OpenTTDLab’s default number of
workers, the number of cores on the machine, which in this case were 8. The Python code to run the experiments is in Appendix \ref{chapter:own-parameterised-ai-run-experiments} and to analyse the results in Appendix \ref{chapter:own-parameterised-ai-analyse-results}.

\section{Results}

As can be seen in Figure \ref{fig:simple-parameterised-ai-means}, there appears to be a clear trend in terms of mean money in the bank at the end of the 50 years: the more buses, the more money in the bank. However, before approximately 1975 the trend was approximately other way around: from starting at £100k in the bank, the money in the bank decreased for all configurations, but the more buses the less money in the bank. Interestingly at approximately 1975 there is what seems to be a rough convergence point: all configurations have approximately the same amount of money in the bank.

\begin{figure}[p]
\centering
\begin{gnuplot}[terminal=cairolatex,terminaloptions={size 5,3}]
set datafile separator ","
set style fill pattern 2
set key left top
set key invert
set grid ytics
set format y "%.0s%c"
set xtics 3600*24*365.25*5
set xdata time
set format x "%Y"
set timefmt "%Y-%m-%d"
set ylabel 'Money in the bank $\textrm{\pounds}$'
set xlabel 'Date'
set xrange ["1949-04-01":"2000-12-01"]
plot 'notebooks/02_own_parameterised_ai_results_02_money_mean_standard_deviation.csv' \ 
   using (timecolumn(1, '%Y-%m-%d')):6 skip 2 with lines title 'Mean for 16 buses', \
   '' using (timecolumn(1, '%Y-%m-%d')):5 skip 2 with lines title 'Mean for 8 buses', \
   '' using (timecolumn(1, '%Y-%m-%d')):4 skip 2 with lines title 'Mean for 4 buses', \
   '' using (timecolumn(1, '%Y-%m-%d')):3 skip 2 with lines title 'Mean for 2 buses', \
   '' using (timecolumn(1, '%Y-%m-%d')):2 skip 2 with lines title 'Mean for 1 bus'
\end{gnuplot}
\caption{How the mean money in the bank changes over time for the AI of Algorithm \ref{algorithm:simple-parameterised-ai} when run for 1, 2, 4, 8 and 16 buses.}
\label{fig:simple-parameterised-ai-means}
\end{figure}

\begin{figure}[p]
\centering
\begin{gnuplot}[terminal=cairolatex,terminaloptions={size 5,3}]
set datafile separator ","
set style fill pattern 2
set key left top
set key invert
set grid ytics
set format y "%.0s%c"
set xtics 3600*24*365.25*5
set xdata time
set format x "%Y"
set timefmt "%Y-%m-%d"
set ylabel 'Money in the bank $\textrm{\pounds}$'
set xlabel 'Date'
set xrange ["1949-04-01":"2000-12-01"]
plot 'notebooks/02_own_parameterised_ai_results_02_money_mean_standard_deviation.csv' \ 
   using (timecolumn(1, '%Y-%m-%d')):($6+$10):($6-$10) skip 2 with filledcurves lc rgb '#cccccc' title "$\\pm \\textrm{1 std. dev.}$", \
   '' using (timecolumn(1, '%Y-%m-%d')):6 skip 2 with lines title 'Mean for 16 buses', \
   '' using (timecolumn(1, '%Y-%m-%d')):($6+$10) skip 2 with lines lc rgb '#cccccc' title '', \
   '' using (timecolumn(1, '%Y-%m-%d')):($6-$10) skip 2 with lines lc rgb '#cccccc' title '', \
   '' using (timecolumn(1, '%Y-%m-%d')):($2+$6):($2-$6) skip 2 with filledcurves lc rgb '#cccccc' title "$\\pm \\textrm{1 std. dev.}$", \
   '' using (timecolumn(1, '%Y-%m-%d')):2 skip 2 with lines title 'Mean for 1 bus', \
   '' using (timecolumn(1, '%Y-%m-%d')):($2+$6) skip 2 with lines lc rgb '#cccccc' title '', \
   '' using (timecolumn(1, '%Y-%m-%d')):($2-$6) skip 2 with lines lc rgb '#cccccc' title ''
\end{gnuplot}
\caption{How the distribution of money in the bank changes over time for the AI of Algorithm \ref{algorithm:simple-parameterised-ai} when run for 1 and 16 buses.}
\label{fig:simple-parameterised-ai-distributions}
\end{figure}

How the distributions of money in the bank change over time for the experiments with one bus compared with 16 buses can be seen in Figure \ref{fig:simple-parameterised-ai-distributions}. For the majority of the time range the distribution for 16 buses is strictly wider than that for 1 buses; although towards the end of the time range the lower end converge.

\section{Discussion}

The results appear to show two things. Firstly, the apparently flip of trends between 1975 and after 1975 suggest that in this situation, it shows that investing money in the short term can lead to more money in the bank longer term. Secondly, the overlapping distributions suggest that the situation is more complex: while the mean for 16 buses is higher, the lower side of the distribution is lower than for 1 buses, which can be interpreted as there being a greater risk of making less money with 16 buses than on 1. These mean that OpenTTD, via OpenTTDLab, can be used to investigate the longer term consequences of shorter term investments, and can be used to investigate risk-benefit trade offs.

Unfortunately the convergence point around 1975 is so far unexplained. It is a convergence of only 4 experiments, maybe not enough to conclude it really is a convergence point. However, further investigation with the data that is available in OpenTTDLab may suggest answers.

These are of course limited results, with no reference or comparison to any formal economic models. The AI constructed is extremely simple, and the infrastructure it creates and uses borderline does not meet the definition of a transport network. But in spite of this simplicity, the results show some interesting effects, and I would argue interesting enough for further research. And research where, with quite a small amount of code as can be seen in Appendices \ref{chapter:own-parameterised-ai-run-experiments} \& \ref{chapter:own-parameterised-ai-analyse-results}, OpenTTDLab should be able to be used.

\section{Conclusion}

Through simulating basic transportation networks using OpenTTDLab, OpenTTD can produce some compelling results that appear to have real-world meanings. As described in Chapter \ref{chapter:openttd-model-and-abilities} OpenTTD's economic and transportation models are limited, so it remains to be seen if these can offer any real-world insight. However, even if this is not the case, OpenTTDLab can be used to research algorithms: in this case the effects of changing a single parameter of an algorithm has interesting effects in the world of OpenTTD. And finally, OpenTTDLab can at the very least be used to run regression tests of OpenTTD AIs, which lends weight to it being a reusable tool.

\chapter{Experiments 3: Scaling}
\label{chapter:experiments-scaling}

As mentioned in Chapter \ref{chapter:openttdlab-design-process-and-features}, in order to make it easier to repeat experiments by reducing their cost in terms of time, I added two features to OpenTTDLab: simple task-based parallelism and a mechanism to reduce the amount of data copied between the worker and the main process. To validate these features, and to start to investigate how OpenTTDLab could be further improved on these fronts, I ran a set of basic experiments: measuring how the wallclock runtime of OpenTTDLab changes with the number of worker processes, and how it changes with the amount of data passed between processes.

\section{Experimental setup}

All the simulations were conducted on a 2020 Apple M1 with 8 CPU cores, 16GB of RAM, OpenTTDLab 0.0.72, OpenTTD 13.4, Python 3.11.0, trAIns 2.1 retrieved from the BaNaNaS content service, and running for seeds 0 to 49, and each for a total of 1465 in-game days (just over 4 in-game years). The Python code to run the experiments can be seen in Appendix \ref{chapter:scaling-running-code}, and the code to subsequently process and visualise the results in Appendix \ref{chapter:scaling-analyis-code}.

The number of worker processes was varied between 1 and 8, and the amount of data was either all the data parsed from the OpenTTD savegame files, and what I class is a minimal set of data to be useful: the date, the company value at that date, and if an error occurred.

\section{Results}

\begin{figure}[p]
\centering
\begin{gnuplot}[terminal=cairolatex,terminaloptions={size 5,3}]
set datafile separator ","
set style fill pattern 2
set key left top
set key invert
set grid ytics
set ylabel 'Time (seconds)'
set yrange [0:]
set xlabel 'Number of worker processes'
plot 'notebooks/03_scaling_results_02_wallclock_times.csv' \ 
   using 1:2 with linespoints ls 1 title 'All data', \
   '' using 1:3 with linespoints ls 2 title 'Minimal data'
\end{gnuplot}
\caption{Total wallclock time of when running 50 experiments using OpenTTDLab between 1 and 8 worker processes, comparing returning all data to the controlling process, and returning a minimal amount of data.}
\label{figure:scaling-wallclock-time}
\end{figure}

\begin{figure}[p]
\centering
\begin{gnuplot}[terminal=cairolatex,terminaloptions={size 5,3}]
set datafile separator ","
set style fill pattern 2
set key left top
set key invert
set grid ytics
set ylabel 'Speedup'
set yrange [1:]
set xlabel 'Number of worker processes'
plot 'notebooks/03_scaling_results_03_speedups.csv' \ 
   using 1:4 with linespoints ls 3 title 'Linear scaling', \
   '' using 1:2 with linespoints ls 1 title 'All data', \
   '' using 1:3 with linespoints ls 2 title 'Minimal data'
\end{gnuplot}
\caption{Speedup when running 50 experiments using OpenTTDLab between 1 and 8 worker processes, comparing returning all data to the controlling process, and returning a minimal amount of data.}
\label{figure:scaling-speedup}
\end{figure}

As can be seen from Figure \ref{figure:scaling-wallclock-time}, wallclock time decreases as the number of processors increases, and so there is some useful parallelisation: running with 8 worker processes results in an 80\% reduction of runtime compared to running on 1 process. Also returning a minimal set of data from the worker resulted in an approximately 30\% reduction in time. One can conclude that it's worth running them on multiple workers, and to reduce the amount of data returned from the workers, especially if running a high number of experiments.

However, Figure \ref{figure:scaling-wallclock-time} also suggest that that there is an aspect of diminishing returns: for example the difference in runtime between 1 and 4 processes is much greater than the difference between 5 and 8. To better show this effect, we investigate \emph{speedup}: how many times faster the program runs for a given number of worker processes compared to the time it takes running on a single worker process. The speedup for the experiments, together with a theoretical linear speedup can be seen in Figure \ref{figure:scaling-speedup}.

It can be seen that the speedup does increase with each addition of a process. However, the greater the number of processes, the greater the deviation from linear speedup. It gets especially worse from 5 processes onwards, ending with a speedup of approximately 4.5 with 8 processes. It can also be seen that the speedup profiles of returning all data and minimal data from the worker processes is almost identical.

\section{Discussion}

A reason for the greater-difference-from-linear scaling when the number of processors is greater than 4 could come from that the cores are not identical. According to Apple \cite{AppleM1Overview} 4 cores are high performance, but the other 4 are lower performance (but have higher efficiency in terms of power usage). The scaling profiles found appears roughly consistent with the high/lower performance split, and suggests that the high performance cores being used in preference to the low performance cores.

Another reason for the deviation from linear speedup is Amdhal's Law that describes how speedup is related to proportion of the program that runs in serial. This law is usually formulated as

\begin{equation}
S_p = \frac{1}{(1-p) + \frac{p}{s}}
\end{equation}
%
where $S_p$ is the speedup of the entire program, $p$ is the proportion of the program that has been parallelised, and $s$ is speedup of the parallelised part of the program. The most relevant consequence of this is that $\lim_{S\to\infty} S_p = \frac{1}{1-p}$. This means that, for example, even if 95\% of a program is parallelised, then the maximum speedup is $\frac{1}{1-0.95}=20$, even if thousands of processes are used.

This suggests that a way of increasing this limit is to reduce proportion of runtime in serial code. However, in light of Amdhal's law the identical scaling profiles in Figure \ref{figure:scaling-speedup} suggest that changing the amount of data returned from the worker processes does not change the overall proportion of runtime in serial code. Given wallclock time reduces with less data transferred, it's reasonable to suspect that reducing the data transferred reduces parallel and serial runtimes in the same proportions, possibly from the serialisation and deserialisation involved. Thus removing all data transfer is not likely to improve scaling. Also, one way to remove all data transfer is to use threads rather than processes, but threads in Python are subject to the Global Intepreter Lock (GIL), and so in many cases increases the proportion of serial code compared to using processes, there is a strong reason to suspect that this will make scaling worse, not better.

So a reasonable next step would be to investigate what else in the program could be running serially. This is potentially not a trivial project---for example it might be from what appears to be parallel sections of the code when examining the algorithm or the Python code, but it accesses a shared resource under contention from the other processes, perhaps even indirectly. As examples, in general there some types of memory caches are shared between cores, or reading or writing to disk  may also effectively be done serially, or at least not fully parallel up to the number of cores. To understand and improve upon the current behaviour this would need a more in-depth understanding of the computer architecture being run on, and profiling of the Python code to understand where time is being spent. This is beyond the scope of the project and so is left to further work.

\section{Conclusion}

The conclusions we can draw here are that OpenTTDLab can utilise multiple cores to reduce runtime of simulations, albeit imperfectly. Even though reducing the amount of data returned from the workers does not improve scaling, it can be an effective way to reduce wallclock runtime, and help repeatability: an aim of OpenTTDLab.

\chapter{Discussion}
\label{chapter:discussion}

As discussed in Chapter \ref{chapter:introduction}, OpenTTD has been used to run experiments to help research a variety of algorithms. However, from my own reviews not a single such study has been found that does not appear to have problems in at least one of the 3Rs of repeatability, reproducibility, or replicability; I classed these problems into 7 issues. Given the wider replication crisis in many fields of science, this is a trend that should not continue. The framework created as part of this work, OpenTTDLab, seeks to address these 7 issues by making it straightforward to repeat experiments using OpenTTD; and if artifacts are shared, straightfoward for others to reproduce them; and if details of the algorithm(s) of AIs are shared, straightforward to replicate them. Put another way, while OpenTTDLab is useful in terms of the 3Rs, simply using OpenTTDLab is not enough to achieve the 3Rs: authors must take separate steps such as publishing artifacts and making sure algorithms are sufficiently described; the documentation of OpenTTDLab makes suggestions on these fronts.

OpenTTD itself provides a rich and interesting setting for studies of algorithms. Its Squirrel-based AI framework, coupled with its tick-based and so deterministic simulator as described in Chapter \ref{chapter:openttd-model-and-abilities}, provides an excellent foundation for bitwise repeatable and reproducible studies, and if presented with sufficient detail, replicable studies as well. While the supply chain of OpenTTD is clearly a simplification of reality, and its economic model is unrealistic, it has been suggested there are realistic aspects to OpenTTD in terms of network effects \cite{raghothama2013review}; I did not attempt to validate OpenTTD as a simulator, but I have found nothing to contradict this suggestion. If OpenTTDLab indeed provides a good framework for the 3Rs as I claim it does, it puts further research that uses OpenTTD as a simulator of real world networks in good stead.

From the first set of experiments, those in Chapter \ref{chapter:experiments-attempt-at-reproducing}, I have given stronger evidence of the difficulty of reproducing OpenTTD results: I successfully generated results, but they did not fully match the ones originally reported. However, by providing the artifacts for running experiments and analysing their results, I have shown that by using OpenTTDLab it is straightforward to repeat experiments, and argue that that others should be able to bitwise reproduce the results I have.

The second set of experiments, in Chapter \ref{chapter:experiments-simple-parameterised-ai}, I have again shown again that OpenTTDLab can be used to easily repeat experiments. But going further, the results of these experiments suggest that OpenTTD could be used to simulate real-world properties relating to transportation, business, or logistics; by parameterising the AI written for the experiments, my results appear to show risk-benefit trade offs and the suggestion that more money initially invested can result in higher returns later. This is through an extremely simple AI that takes an extremely narrow range of possible actions an OpenTTD AI can take, so I strongly suspect there is a wealth of insights that can be discovered by writing more complex AIs that take a wider range of actions.

The third set of experiments in Chapter \ref{chapter:experiments-scaling} show that OpenTTDLab can run experiments in parallel, leveraging the multiple CPUs on a single laptop computer, albeit with poor scaling. This allows for experiments to be repeated more times within the same time constraints, and so allows research with results that have more evidential weight behind them than they otherwise would.

These three sets of experiments are different: one attempts to replicate an existing study, one parameterises an newly written AI, and one investigates the scaling properties of OpenTTDLab itself. This difference itself provides evidence that OpenTTDLab is itself reusable, and by being reusable increases the reproducibility of studies that use it \cite{benureau2018re}. Also for all three of the experiments, the artifacts to run the experiments and those to analyse their results are included in appendices, and because they delegate to OpenTTDLab, are relatively short in terms of numbers of Python statements. This suggests that it should be feasible to include such artifacts in published studies, or at least provide permanent links to them published separately, again showing that OpenTTDLab should be able to be used in future reproducible work.

I developed OpenTTDLab using the highly agile mechanism as described in Chapter~\ref{chapter:openttdlab-design-process-and-features}, iteratively writing and using the framework to create the results of Chapters \ref{chapter:experiments-attempt-at-reproducing}--\ref{chapter:experiments-scaling} while attempting to avoid the 7 issues described in Chapter \ref{chapter:introduction}; this process itself provides a good reason to believe it is fit for purpose and achieves the aims of helping to achieve the 3Rs. However, according to ACM's definitions, the 3Rs are each the domain of multiple sets of authors: those originally conducting the research and those then attempting to reproduce or replicate it. To fully validate that OpenTTDLab can be a useful tool in these endeavors, studies that use OpenTTDLab will have to be attempted to be reproduced and replicated, much like the systematic attempts reported by Luijken et al. \cite{luijken2024replicability}. I have provided three sets of experiments in this work, and argued that they should be reproducible and replicable, but the true test of these properties are real attempts by other authors. However, as mentioned in Chapter~\ref{chapter:introduction} on the replication front there are ``no set criteria to assess the alignment of replicated simulation results with the original results'' \cite{luijken2024replicability}, exactly what this would look like for replication is not known. However, since repeatabilty and reproducibility are de-facto precursors to replication, OpenTTDLab is valuable for replication in spite of this uncertainty.

Finally there are numerous features in OpenTTDLab itself that can be improved upon. Improving scalability has already been suggested, but there are non-performance features as well. For example, while OpenTTDLab can generate screenshots of the map of the field of play, these screenshots cannot currently be used in a way suitable for analysis: the map, which includes how roads and railways are connected connected, I suspect would be important in order to use OpenTTD to study network effects. And while in many ways the format is  self-documenting, beyond the examples that extract company value, loan amount and money in the bank, there are many other values, but no standalone exhaustive documentation of what can be extracted from OpenTTD using OpenTTDLab currently exists. In addition, the results from OpenTTDLab are ephemeral: they exist only in memory and designing a format to persisting them to disk I deemed outside the scope of the project, because how to do so is not trivial.

\chapter{Conclusion}
\label{chapter:conclusion}

While there are many improvements that can be made in OpenTTDLab and further investigations performed in terms of whether it does in fact achieve its aims, through this work I have provided evidence that OpenTTDLab provides the foundations of a reusable framework for conducting repeatable, reproducible, and replicable research using OpenTTD. And I have provided evidence that even through even simple experiments, OpenTTD can produce interesting results that appear to have real-world meanings. OpenTTDLab is already a useful tool, and am extremely optimistic in how it can help further research using OpenTTD.

% \chapter{Notes}

% History

% \begin{itemize}

% \begin{item}
% \cite{jackson1959learning} first business simulation game, US Air force MONOPOLOGS pretend to be inventory managers
% \end{item}

% \begin{item}
% \cite{meijer2009organisation} - Studying supply chains using gaming simulation
% \end{item}

% \begin{item}
% \cite{mayer2009gaming} - Defines simulation game as a game "experi(m)ent(i)al, rule-based, interactive environments, where players learn by taking actions and by experiencing their effects through feedback mechanisms that are deliberately built into and around the game"
% \end{item}

% \begin{item}
% \cite{raghothama2013review} war gaming roots, policy analysis, business simulation games, studies on human cognition and behavior, training and pedagogical tools

% Main point: simulation games particular suited for transportation research in general, and supply chain specifically because it's an emergent property

% Some features of OpenTTD realistic, but this isn't exactly justified

% Simutrans mentioned - described as not being realistic at all, but is extensible so could be

% But similuation games not used for transportation

% Gives lots of examples of simulation games used for research, but also mentions "Little attention has been paid to the validation of these simulation games."

% (But: by and large here we're not concerning ourselves with validation)
% \end{item}

% \begin{item}
% Basically have a list of a good range of how similuation games have been used.
% \end{item}

% \begin{item}
% \cite{alderliesten2019maintrain} The simulation game itself used to try to get sympathy/understanding from passengers about delays
% \end{item}

% \begin{item}
% \cite{cimellaro2016computational} - it lists OpenTTD alongside "real" simulators of cities in discussion for disaster reliance, with no particular mention that's it a game/for entertainment??
% \end{item}

% Repeatability


% \begin{itemize}
% \begin{item}
% \cite{dalle2012reproducibility} in simulation in particular "many published works based on simulation still fail to meet the minimum conditions to ensure reproducibility" (and in my opinion the OpenTTD ones are no exception). And it gives some "levels" L1, L2, L3, L4 on how to judge "how" reproducbiel simulations are and highlights specific desirable details about simulations. I can probably judge some existing research using this, and compare to maybe output using my OpenTTDLab thing?

% Hidden details that prevent reproducilibty (it cites another source for this)

% I quite like this one! (TODO Check things that cite it especially)

% "Goes beyond" reproducibility into traceability...

% "– a simulator refers to a reusable simulation engine and its Application Programming Interface
% (API); the engine is reusable for the simulation of many models and scenarios;" OpenTTDLab I think is the "missing" simulator??

% "In simulation, not only the same sources of error
% exist, but the system itself can be modeled incorrectly and be a source of error"

% "Furthermore, as will be explained in greater details
% in the next section, simulation has many applications that are not aimed toward producing science
% and, therefore, do not necessarily require a reproducibility based on the source code availability."

% "• manipulation errors: These errors result from the manual handling of some of the tasks in the simulation study work-flow."

% OpenTTDLab I think addresses loads of the issues mentioned, in many of the ways it suggests.
% \end{item}


% \begin{item}
% \cite{monks2019strengthening} - Checklist style "STRESS" guidelines. Strengthening the Reporting of Empirical Simulation Studies

% It also has a review of lots of other guidelines for reproducible simulations

% TODO: if going this way, reflect on the bits that OpenTTDLab does not do

% "Agent-Based Simulation (ABS), Discrete-Event Simulation (DES) and System Dynamics (SD)." - er... OpenTTD is which of these??

% "it is critical that author report the software version and build numbers" - this is for commercial software, but I would argue for Open Source as well
% \end{item}

% \begin{item}
% Gass (1984) provides the earliest example of reporting guidelines for “computer based models”.
% \end{item}
% \end{itemize}

% \end{itemize}

% \cleardoublepage
% \phantomsection
% \addcontentsline{toc}{chapter}{Bibliography}
% \printbibliography[heading=bibintoc]
% \markboth{Bibliographysdff}{}

% \cleardoublepage
% \phantomsection
% \addcontentsline{toc}{chapter}{Bibliographysdf}

\printbibliography[heading=bibliography,title={Bibliography}]


% You may delete everything from \appendix up to \end{document} if you don't need it.
\appendix

\chapter{Documentation of OpenTTDLab}
\label{chapter:openttdlab-documentation}

\includepdf[pages=-,width=\columnwidth,offset=0.8cm 0, pagecommand={},frame]{assets/openttdlab-github.pdf}

\chapter{Reproducing results: Python code to run experiments}
\label{chapter:trains-vs-admiral-run-experiments}

The Python notebook used to run the trAIns vs Admiral experiments of Chapter \ref{chapter:experiments-attempt-at-reproducing} is below. This latest version is also available at \url{https://github.com/michalc/openttd-msc-dissertation/blob/main/notebooks/01_trains_ai_vs_admiral_ai_01_run_experiments.ipynb}

\includepdf[pages=-,width=\columnwidth,offset=0.8cm 0,pagecommand={},frame]{notebooks/01_trains_ai_vs_admiral_ai_01_run_experiments.pdf}

\chapter{Reproducing results: Python code to analyse results}
\label{chapter:trains-vs-admiral-analyse-results}

The Python notebook used to analyse results for the trAIns vs Admiral experiments of Chapter \ref{chapter:experiments-attempt-at-reproducing} is below. This latest version is also available at \url{https://github.com/michalc/openttd-msc-dissertation/blob/main/notebooks/01_trains_ai_vs_admiral_ai_01_analyse_results.ipynb}

\includepdf[pages=-,width=\columnwidth,offset=0.8cm 0,pagecommand={},frame]{notebooks/01_trains_ai_vs_admiral_ai_02_analyse_results.pdf}

\chapter{Simulating a network: Squirrel `info'}
\label{chapter:simulating-a-network-squirrel-info}

\lstinputlisting[language=Java]{assets/parameterised-ai/info.nut}

\chapter{Simulating a network: Squirrel `main'}
\label{chapter:simulating-a-network-squirrel-main}

\lstinputlisting[language=Java]{assets/parameterised-ai/main.nut}

\chapter{Simulating a network: Python regression test}
\label{chapter:simulating-a-network-python-regression-test}

\lstinputlisting[language=Python]{assets/parameterised-ai/test_parameterised_ai.py}

\chapter{Simulating a network: Python code to run experiments}
\label{chapter:own-parameterised-ai-run-experiments}

The Python notebook used to run the simple parameterised AI experiments of Chapter \ref{chapter:experiments-simple-parameterised-ai} is below. This latest version is also available at \url{https://github.com/michalc/openttd-msc-dissertation/blob/main/notebooks/02_own_parameterised_ai_01_run_experiments.ipynb}.

\includepdf[pages=-,width=\columnwidth,offset=0.8cm 0,pagecommand={},frame]{notebooks/02_own_parameterised_ai_01_run_experiments.pdf}

\chapter{Simulating a network: Python code to analyse results}
\label{chapter:own-parameterised-ai-analyse-results}

The Python notebook used to analyse results for the simple parameterised AI experiments of Chapter \ref{chapter:experiments-simple-parameterised-ai} is below. This latest version is also available at \url{https://github.com/michalc/openttd-msc-dissertation/blob/main/notebooks/02_own_parameterised_ai_02_analyse_results.ipynb}.

\includepdf[pages=-,width=\columnwidth,offset=0.8cm 0,pagecommand={},frame]{notebooks/02_own_parameterised_ai_02_analyse_results.pdf}


\chapter{Scaling: Python code to run experiments}
\label{chapter:scaling-running-code}

The Python notebook used to generate results for the Scaling experiments of XX is below. This latest version is also available at \url{https://github.com/michalc/openttd-msc-dissertation/blob/main/notebooks/03_scaling_01_run_experiment.ipynb}

\includepdf[pages=-,width=\columnwidth,offset=0.8cm 0,pagecommand={},frame]{notebooks/03_scaling_01_run_experiment.pdf}

\chapter{Scaling: Python code to analyse results}

\label{chapter:scaling-analyis-code}

The Python notebook used to analyse results for the Scaling experiments of XX is below. This latest version is also available at \url{https://github.com/michalc/openttd-msc-dissertation/blob/main/notebooks/03_scaling_02_analyse_results.ipynb}

\includepdf[pages=-,width=\columnwidth,offset=0.8cm 0,pagecommand={},frame]{notebooks/03_scaling_02_analyse_results.pdf}

\end{document}
